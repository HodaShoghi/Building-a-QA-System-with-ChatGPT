{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04112a2a",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "## Building a Question Answering System\n",
    "\n",
    "1. [Introduction](#Building-a-Question-Answering-System:)\n",
    "   1. [Our Primary Aim](#Introduction)\n",
    "2. [Synergizing Components for an Effective System](#Synergizing-Components-for-an-Effective-System)\n",
    "   1. [System Overview](#Overview)\n",
    "   2. [Embeddings: The Core](#Embeddings:-The-Core)\n",
    "   3. [Vector Databases: Enhancing Retrieval Efficiency](#Vector-Databases:-Enhancing-Retrieval-Efficiency)\n",
    "   4. [Langchain Library: Simplifying Language Tasks](#Langchain-Library:-Simplifying-Language-Tasks)\n",
    "   5. [Prompt Engineering: Directing ChatGPT](#Prompt-Engineering:-Directing-ChatGPT)\n",
    "   6. [ChatGPT: The Conversational Heart](#ChatGPT:-The-Conversational-Heart)\n",
    "\n",
    "3. [Seamless Workflow Overview](#Seamless-Workflow-Overview)\n",
    "\n",
    "4. [Project Steps Breakdown](#Project-Steps-Breakdown)\n",
    "   1. [Step 1: Importing Libraries](#Step-1:-Importing-Libraries)\n",
    "   2. [Step 2: PDF Text Extraction](#Step-2:-PDF-Text-Extraction)\n",
    "   3. [Step 3: Preparing for Subsequent Steps](#Step-3:-Preparing-for-Subsequent-Steps)\n",
    "   4. [Step 4: Initializing Document Loader](#Step-4:-Initializing-Document-Loader)\n",
    "   5. [Step 5: Preparing Text Splitter](#Step-5:-Preparing-Text-Splitter)\n",
    "   6. [Step 6: Text Splitting Process](#Step-6:-Text-Splitting-Process)\n",
    "   7. [Step 7: API Key Setup](#Step-7:-API-Key-Setup)\n",
    "   8. [Step 8: Creating Chroma Vector Store](#Step-8:-Creating-Chroma-Vector-Store)\n",
    "   9. [Step 9: Setting up VectorDBQA](#Step-9:-Setting-up-VectorDBQA)\n",
    "   10. [Step 10: Executing the First Query](#Step-10:-Executing-the-First-Query)\n",
    "   11. [Step 11: Conducting Additional Queries](#Step-11:-Conducting-Additional-Queries)\n",
    "\n",
    "5. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a3718",
   "metadata": {},
   "source": [
    "# Building a Question Answering System: \n",
    "### Utilizing ChatGPT to Answer Questions Based on Your Personal Data\n",
    "## Introduction\n",
    "\n",
    "\n",
    "Our primary aim is to develop a system capable of effectively answering a wide range of questions, regardless of their complexity. This challenge involves working with data, language understanding, and delivering user-friendly responses.\n",
    "\n",
    "Imagine being able to request key information from an email thread or locate specific details within your company's documents effortlessly. This is made possible by harnessing the power of embeddings, vectors, vector databases, and prompt engineering in conjunction with large language models like ChatGPT.\n",
    "\n",
    "This system refines user queries into structured prompts, amalgamating the query with relevant text passages from the vector database. This approach furnishes ChatGPT with a rich context, enhancing the relevance and accuracy of its responses.\n",
    "\n",
    "\n",
    "\n",
    "# Synergizing Components for an Effective System\n",
    "\n",
    "## Overview\n",
    "\n",
    "Our quest to develop a robust question-answering system integrates embeddings, vector databases, the Langchain library, prompt engineering, and ChatGPT.\n",
    "In the context of the question-answering system we build, the answer to a user's query is found in the vector database that was created from our data file. The system uses vector search techniques to identify the most relevant text passages or documents in the database that match the user's query. It then combines this relevant information with the user's query to create a prompt for ChatGPT, which generates a response based on its general knowledge and language abilities.\n",
    "\n",
    "Let's delve into how these elements synergize.\n",
    "\n",
    "### Embeddings: The Core\n",
    "\n",
    "Embeddings are central to our system, converting text into numerical representations. They serve as the bridge between language and mathematics, facilitating analysis and processing.\n",
    "\n",
    "### Vector Databases: Enhancing Retrieval Efficiency\n",
    "\n",
    "Vector databases are pivotal for storing and retrieving embeddings. They optimize searches in high-dimensional spaces, ensuring fast and precise results from user queries.\n",
    "Vectors, or numerical text fingerprints, transform words or documents into numerical arrays, capturing their essence. These embeddings, akin to stars in a galaxy, are positioned in a mathematical space, signifying textual similarities and differences. Closer points indicate related texts, making information retrieval efficient and relevant.\n",
    "\n",
    "### Langchain Library: Simplifying Language Tasks\n",
    "\n",
    "Langchain aids in text processing and transforming text into embeddings, streamlining complex language operations.\n",
    "\n",
    "### Prompt Engineering: Directing ChatGPT\n",
    "\n",
    "Crafting structured prompts merges user queries with relevant text, providing ChatGPT with direction for generating contextually relevant responses.\n",
    "\n",
    "### ChatGPT: The Conversational Heart\n",
    "\n",
    "ChatGPT, the core language model, processes these prompts to deliver human-like, coherent responses, making the system interactive and user-friendly.\n",
    "\n",
    "\n",
    "## Seamless Workflow Overview\n",
    "\n",
    "1. **Embedding Generation**: Langchain converts text into numerical embeddings.\n",
    "2. **Vector Database Storage**: These embeddings are stored for optimized retrieval.\n",
    "3. **Query Processing**: User queries are transformed into embeddings.\n",
    "4. **Similarity Search**: The system identifies relevant matches for the query embedding.\n",
    "5. **Prompt Crafting**: A context-rich prompt is formulated.\n",
    "6. **Response Generation**: ChatGPT interprets the prompt to generate a relevant response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d8ec0",
   "metadata": {},
   "source": [
    "# Project Steps Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9ef92",
   "metadata": {},
   "source": [
    "### Step 1: Importing Libraries\n",
    "I started by install and importing the libraries I needed for my project.\n",
    "\n",
    "##### pip install requests PyPDF2:\n",
    "This installs requests for making HTTP requests (to download the PDF file) and PyPDF2 for handling PDF files.\n",
    "##### pip install langchain:\n",
    "Installs the Langchain library. \n",
    "##### pip install openai:\n",
    "Installs the OpenAI library, which is needed for the embeddings and chat models you're using in Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2c852",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: PDF Text Extraction\n",
    "\n",
    "I defined a function called `extract_text_from_pdf` to extract text from a PDF using its URL. This function sends a GET request to the PDF URL, saves the PDF content to a local file, and then uses PyPDF2 to extract text from the PDF. The extracted text is saved to a text file.Then I printed a message to confirm that the extracted text has been successfully saved to \"ml_competition.txt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491420ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to ml_competition.txt\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "# Define a function to extract text from a PDF given its URL\n",
    "def extract_text_from_pdf(pdf_url):\n",
    "    # Send a GET request to the PDF URL and store the response\n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    # Save the PDF content to a local file named \"ML-Competition.pdf\"\n",
    "    with open(\"ML-Competition.pdf\", \"wb\") as pdf_file:\n",
    "        pdf_file.write(response.content)\n",
    "    \n",
    "    # Open the saved PDF file for reading\n",
    "    pdf_file = open(\"ML-Competition.pdf\", \"rb\")\n",
    "    \n",
    "    # Create a PdfFileReader object to read the PDF\n",
    "    pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "    \n",
    "    # Initialize an empty string to store extracted text\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Loop through each page in the PDF and extract text\n",
    "    for page_num in range(pdf_reader.numPages):\n",
    "        page = pdf_reader.getPage(page_num)\n",
    "        extracted_text += page.extractText()\n",
    "    \n",
    "    # Close the PDF file\n",
    "    pdf_file.close()\n",
    "    \n",
    "    # Return the extracted text\n",
    "    return extracted_text\n",
    "\n",
    "# Call the function with a PDF URL and save the extracted text to a text file\n",
    "pdf_text = extract_text_from_pdf(\"https://arxiv.org/pdf/2310.02263.pdf\")\n",
    "file_path = \"ml_competition.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(pdf_text)\n",
    "\n",
    "# Print a message indicating where the extracted text is saved\n",
    "print(f\"Extracted text saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a918b5e",
   "metadata": {},
   "source": [
    "### Step 3: Preparing for Subsequent Steps\n",
    "\n",
    "I imported additional libraries and modules that I'll need for the next parts of my project, which involve working with the extracted text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f267c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the next part\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA, VectorDBQA\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faf2e1",
   "metadata": {},
   "source": [
    "### Step 4: Initializing Document Loader\n",
    "\n",
    "I initialized a document loader using the extracted text file \"ml_competition.txt.\"\n",
    "first initializing a document loader to read data from a text file ('ml_competition.txt') and then actually loading this data, storing it in a variable for subsequent use in the program. This is a typical pattern in data processing where you first set up a mechanism to read data (a loader) and then use it to actually get the data into your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2691f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a document loader with the extracted text file\n",
    "loader = UnstructuredFileLoader('ml_competition.txt')\n",
    "\n",
    "# Load documents using the document loader\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49ff73",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Preparing Text Splitter\n",
    "\n",
    "I initialized a text splitter object with specific configuration settings for how text should be divided into chunks. This object is then used for processing the text data in smaller, more manageable parts.\n",
    "\n",
    "**Initialize Text Splitter**:\n",
    "   - `text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)`\n",
    "   - This line creates a `CharacterTextSplitter` object, which is used to split text into smaller parts. The `chunk_size` parameter is set to 1000, indicating each text chunk will contain approximately 1000 characters. The `chunk_overlap` parameter is set to 0, meaning there will be no overlap between consecutive text chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dd8a4",
   "metadata": {},
   "source": [
    "### Step 6: Text Splitting Process\n",
    "\n",
    "\n",
    "**Split Documents into Chunks**:\n",
    "   - `texts = text_splitter.split_documents(documents)`\n",
    "   - This line applies the `split_documents` method of the `text_splitter` object to the `documents`. The result is the division of the `documents` text into smaller, more manageable chunks, stored in the `texts` variable. This process makes the data suitable for further analysis or processing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff0b1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into smaller chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393143e7",
   "metadata": {},
   "source": [
    "### Step 7: API Key Setup\n",
    "\n",
    "I set the environment variable OPENAI_API_KEY to a specific value, which is the API key provided by OpenAI.\n",
    "This environment variable is crucial for authenticating requests made to the OpenAI API, such as generating embeddings or interacting with language models like GPT-3 or GPT-4.\n",
    "\n",
    "#### Initializing OpenAI Embeddings\n",
    "I initialized OpenAI embeddings to work with the text data effectively.\n",
    "It creates an instance of the OpenAIEmbeddings class. This class is part of a library that interfaces with OpenAI's services for generating text embeddings.\n",
    "By calling OpenAIEmbeddings(), we are initializing the embeddings generation tool without specifying any additional configuration parameters. It will use default settings.\n",
    "The resultant object, embeddings, is now ready to be used for generating embeddings of text data. Embeddings are numerical representations that capture the semantic essence of the text, which can be used in various natural language processing applications.\n",
    "This step is crucial for preparing the text data for operations like semantic search or similarity analysis in later stages of the pipeline.\n",
    "\n",
    "#### Printing API Key\n",
    "I printed the API key from the environment variable to ensure it was set correctly.\n",
    "It retrieves the value of the environment variable OPENAI_API_KEY using Python's os.environ.get method.\n",
    "This method looks up the value of the environment variable named OPENAI_API_KEY, which was previously set in the script.\n",
    "print is then used to display this value. This can be useful for verification purposes to ensure that the correct API key is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb015da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Set the OPENAI_API_KEY environment variable (replace with your API key)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"******My-API-KEY*****\"\n",
    "\n",
    "# Print the API key from the environment variable\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d82a2",
   "metadata": {},
   "source": [
    "### Step 8: Creating Chroma Vector Store\n",
    "\n",
    "I created a Chroma vector store from the text chunks and embeddings, which enables efficient similarity searches.\n",
    "\n",
    "\n",
    "### Initializing a Chroma Vector Store\n",
    "\n",
    "- **Initializing a Chroma Vector Store**  using the Chroma class. Chroma is a part of a library designed for handling and storing vector representations of text data, commonly used in natural language processing.\n",
    "\n",
    "- **Using the `from_documents` Method**: The `from_documents` method is a static method of the Chroma class. It is used to create a new vector store from a given set of documents or text chunks.\n",
    "\n",
    "- **Input Parameters**:\n",
    "   - `texts`: This variable contains the text data that you want to store in the vector database. It could be raw text data or preprocessed text data. Since the texts were previously split into smaller chunks, `texts` would be a collection of these chunks.\n",
    "   - `embeddings`: This is an instance of an embeddings generator (likely from OpenAI, as suggested by `OpenAIEmbeddings()` used earlier in our code). This generator is used to create embeddings for the text data. Embeddings are numerical representations that capture the semantic meaning of the text.\n",
    "\n",
    "- **Storing in Variable `db`**: The result of `Chroma.from_documents(texts, embeddings)`—the Chroma vector store—is then stored in the variable `db`. This vector store (`db`) can be used for various operations like similarity searches, where you can find the most semantically relevant documents or text chunks for a given query.\n",
    "\n",
    "`Chroma.from_documents(texts, embeddings)` This line of code essentially creates a searchable database of text data, where each piece of text is represented by its semantic embedding, facilitating efficient and semantically driven search and retrieval operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma vector store from the text chunks\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16927226",
   "metadata": {},
   "source": [
    "### Step 9: Setting up VectorDBQA\n",
    "\n",
    "I initialized a VectorDBQA for question-answering purposes, using ChatOpenAI as the language model.\n",
    "\n",
    "### Initializing a Vector-based Question Answering System\n",
    "\n",
    "`qa = VectorDBQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"map_reduce\", vectorstore=db, k=1)`\n",
    "\n",
    "- **Initialize a VectorDBQA**: This line of code is responsible for initializing a Vector-based Question Answering (QA) system. In this context, \"QA\" stands for Question Answering, a natural language processing task where a machine is trained to answer questions posed in human language.\n",
    "\n",
    "- **Input Parameters**:\n",
    "   - `llm=ChatOpenAI()`: Here, `llm` is set to `ChatOpenAI()`, which initializes a Chat-based language model for answering questions. The `ChatOpenAI` model is likely designed to generate text responses based on the input query.\n",
    "   - `chain_type=\"map_reduce\"`: This parameter specifies the type of processing chain used by the QA system. In this case, it's set to \"map_reduce,\" which refer to a specific approach or algorithm used for vector-based question answering.\n",
    "   - `vectorstore=db`: `vectorstore` is set to `db`, which presumably refers to the Chroma vector store created earlier using text data and embeddings.\n",
    "   - `k=1`: This parameter specifies the number of top-k results or answers to return for a given query. In this case, it's set to 1, indicating that the system should provide the top-ranked answer.\n",
    "\n",
    "- **Functionality**:\n",
    "   - This line of code effectively creates a QA system that is designed to answer questions based on a vector store of text data. The QA system uses the specified language model (`ChatOpenAI`), a particular processing chain type, and the vector store (`db`) as a source of information to generate answers.\n",
    "   - The `k` parameter allows us to control how many answers the system should return for each question.\n",
    "\n",
    "This code segment sets up the foundation for a vector-based question answering system, enabling it to respond to questions based on the information stored in the vector database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1548921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a VectorDBQA\n",
    "qa = VectorDBQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"map_reduce\", vectorstore=db, k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4c949",
   "metadata": {},
   "source": [
    "### Step 10: Executing the First Query\n",
    "\n",
    "`query = \"What is the document about\"`\n",
    "`qa.run(query)`\n",
    "\n",
    "- **Define a Query**: In this line of code, a query is defined as a text string. The query serves as the question or input for the Question Answering (QA) system. In this case, the query is set to \"What is the document about.\"\n",
    "\n",
    "- **Run the Query**: The code `qa.run(query)` is responsible for running the defined query through the Question Answering (QA) system. This step triggers the QA system to process the input query and generate a response based on its knowledge and understanding of the provided data.\n",
    "\n",
    "- **Output**: The output of this operation is the response generated by the QA system in answer to the query. The response will typically be a text string that provides an answer or explanation related to the query.\n",
    "\n",
    "- **Usage**: This code segment demonstrates how to use the initialized QA system to obtain answers to specific questions or queries. You can replace the query with different questions to get relevant responses from the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac35ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document is about fine-tuning language models using human preferences.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a query and run it through the QA system\n",
    "query = \"What is the document about\"\n",
    "qa.run(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18488d5",
   "metadata": {},
   "source": [
    "### Step 11: Conducting Additional Queries\n",
    "I defined another query, \"Explain fine-tuning language models in the language of a rapper,\" and ran it through the QA system. The system provided a creative rapper-themed explanation of fine-tuning language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef0b818e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yo, listen up, let me drop some knowledge in a rapper's style\\nFine-tuning language models, it's like takin' somethin' dope and makin' it wild\\nWe start with these Large Language Models, they're the real deal\\nTrained on trillions of tokens, they got that mass appeal\\n\\nBut we ain't satisfied, we want 'em to be the best\\nSo we put 'em to the test, pushin' 'em beyond the rest, no time for rest\\nWe got these procedures, like supervised instruction tuning\\nTeachin' 'em specific tasks, get 'em groovin', no misassumptions\\n\\nThen there's Reinforcement Learning from Human Feedback\\nLet the people guide 'em, give 'em that street cred, that real-life street beat\\nThey learn from feedback, adjustin' and adaptin'\\nSpittin' rhymes that'll blow your mind, no misconceptions, just straight snappin'\\n\\nBut hold up, there's more, we ain't done yet\\nContrastive post-training, yo, don't you forget\\nIt's all about alignment, findin' that perfect match\\nStartin' easy and movin' to the hard batch, ain't no catch\\n\\nAs we scale up, more data, bigger models in the mix\\nLike Orca, performance boostin', droppin' hits like a mix\\nOrca's already a state-of-the-art instruction learner\\nWith contrastive post-training, it's a straight-up burner, makin' heads turner\\n\\nSo to sum it up, fine-tuning language models is the way\\nTo take 'em from good to great, makin' 'em slay\\nBillions of parameters, trillions of tokens, they reign supreme\\nThese models are the kings, rulin' the spoken, livin' the dream.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define another query and run it through the QA system\n",
    "query = \"Explain fine-tuning language models in the language and tone of a rapper\"\n",
    "qa.run(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df0cf1",
   "metadata": {},
   "source": [
    "**Isn't it amazing ?** The answer provided by the system in the language and tone of a rapper is not only creative but also demonstrates the versatility and adaptability of the question-answering system. It seamlessly translates complex concepts like \"fine-tuning language models\" into a hip-hop narrative, making it engaging and easy to understand. This response showcases the system's ability to tailor its answers to the user's specified style and tone, adding a layer of fun and accessibility to the interaction. It exemplifies how AI can bring a touch of creativity and humor to technical explanations, making information more relatable and enjoyable for users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2fa2fe",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In conclusion, our project showcases the power of modern NLP techniques and tools, enabling us to create a versatile question-answering system that can be applied to diverse use cases. By combining embeddings, vector databases, and prompt engineering, we've unlocked the full potential of large language models like ChatGPT, facilitating efficient and semantically driven search and retrieval operations. This project marks a significant stride towards more accessible and powerful natural language understanding and interaction systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_nlp",
   "language": "python",
   "name": "ml_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
